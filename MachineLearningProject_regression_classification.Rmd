---
title: "ML_project"
author: "Gökce Ergün"
date: "12/24/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## R Markdown

Watch out! it takes long time to load this data.
Only load this chunk once...

```{r cars}
file<-"./cars.csv"
cars_data<-read.csv2(file, header = TRUE, sep = ",")
str(cars_data)
```

## Data cleaning

1. Price and engine capacity columns are converted to float. 

```{r}
cars_data$engine_capacity <- as.numeric(as.character(cars_data$engine_capacity))
cars_data$price_usd <- as.numeric(as.character(cars_data$price_usd))
```

2. Dealing with missing values

```{r}
for (i in colnames(cars_data)){
  n_missing = sum(is.na(cars_data[[i]]))
  s_missing = mean(is.na(cars_data[[i]]))
  print(paste(i, "=", n_missing))
}
```

There are only 10 Null values for "Engine Capacity".

Dropping these null values:
  
  ```{r}
cars_data = na.omit(cars_data)
```

3. Calculate the age of the car

```{r}
cars_data$age <- 2020 - cars_data$year_produced
head(cars_data)
```


4. Recode the variables with two level as dummy variable (True-False: 1-0) 

```{r}
cars_data$transmission_is_automatic <- ifelse(cars_data$transmission == 'automatic' , 1, 0)
cars_data$engine_has_gas <- as.numeric(lapply(cars_data$engine_has_gas, as.logical))
cars_data$has_warranty <- as.numeric(lapply(cars_data$has_warranty, as.logical))
cars_data$is_exchangeable   <- as.numeric(lapply(cars_data$is_exchangeable, as.logical))
cars_data$feature_0 <- as.numeric(lapply(cars_data$feature_0, as.logical))
cars_data$feature_1 <- as.numeric(lapply(cars_data$feature_1, as.logical))
cars_data$feature_2 <- as.numeric(lapply(cars_data$feature_2, as.logical))
cars_data$feature_3 <- as.numeric(lapply(cars_data$feature_3, as.logical))
cars_data$feature_4 <- as.numeric(lapply(cars_data$feature_4, as.logical))
cars_data$feature_5 <- as.numeric(lapply(cars_data$feature_5, as.logical))
cars_data$feature_6 <- as.numeric(lapply(cars_data$feature_6, as.logical))
cars_data$feature_7 <- as.numeric(lapply(cars_data$feature_7, as.logical))
cars_data$feature_8 <- as.numeric(lapply(cars_data$feature_8, as.logical))
cars_data$feature_9 <- as.numeric(lapply(cars_data$feature_9, as.logical))
#str(cars_data)
```


## Main Question: 

Find the right model to predict the price of a car. 


## Examining the relationship between predictor and other variables.
(/Identify possible predictors)

### Correlation matrix

Will be used to examine the relationship between the price and other numeric variables.  

1. Select numeric columns

```{r}
names(dplyr::select_if(cars_data,is.numeric))
cars_data_num <- cars_data[c("odometer_value", "engine_capacity", "price_usd", "number_of_photos", "up_counter", "duration_listed","age" )]
col_order <- c("price_usd", "odometer_value", "engine_capacity", "number_of_photos", "up_counter", "duration_listed","age" )
cars_data_num <- cars_data_num[, col_order]
```

2. Correlation matrix

```{r, fig.width = 20, fig.height = 16}
#install.packages("PerformanceAnalytics")
library("PerformanceAnalytics")
chart.Correlation(cars_data_num, histogram=TRUE, pch=19)
```

The correlation matrix shows that price of the car has a strong negative correlation with the age of car, a moderate negative correlation with odometer value, and low to moderate positive correlation with engine capacity and the number of photos that car has. 

The relationship between price and engine capacity looks like linear, and its relationship with other significantly correlated variables are nonlinear. 

```{r}
par(mfrow = c(1, 2))
hist(cars_data$price_usd, breaks = 100, main = "Distribution of the price", xlab="Price")
hist(log(cars_data$price_usd), breaks = 100, main = "Log transformation", xlab="Price")
```

Price of the car is right-skewed and log transformation of it results a normal distribution. Since the linear models assume the normal distribution of the variables, log transformation of the price will be used. 

### Examining the relationship between price and categorical variables


```{r fig.width=20, fig.height=12}
par(mfrow = c(3, 3))
boxplot(log(price_usd)~transmission_is_automatic, data = cars_data)
boxplot(log(price_usd)~color, data = cars_data)
boxplot(log(price_usd)~engine_fuel, data = cars_data)
boxplot(log(price_usd)~engine_has_gas, data = cars_data)
boxplot(log(price_usd)~engine_type, data = cars_data)
boxplot(log(price_usd)~has_warranty, data = cars_data)
boxplot(log(price_usd)~state, data = cars_data)
boxplot(log(price_usd)~drivetrain, data = cars_data)
boxplot(log(price_usd)~is_exchangeable, data = cars_data)
```

It seems that transmission, has_warranty, state of the car, and drivetrain has an effect on price. The price of the cars with some color and engine fuel groups seems also different from the other groups. 


- Recode Manufacturer_name
Luxus cars are decided based on domain knowledge: Acura, Audi, BMW, Cadillac, Infiniti, Jaguar, Land Rover, Lexus, Mercedes-Benz, Porsche, Volvo 
```{r message=FALSE}
library(dplyr)
cars_data <- cars_data %>% mutate(is_luxus = ifelse((manufacturer_name == 'Acura'|manufacturer_name == 'Audi'|manufacturer_name == 'BMW'|
                                                       manufacturer_name == 'Cadillac'|manufacturer_name == 'Infiniti'|manufacturer_name == 'Jaguar'|manufacturer_name ==
                                                       'Land Rover'|manufacturer_name == 'Lexus'|manufacturer_name == 'Mercedes-Benz'|manufacturer_name ==
                                                       'Porsche'|manufacturer_name == 'Volvo'), 1, 0))
```


- Recode Color column 
```{r}
table(cars_data$color)
```

Popular colors are decided based on frequency of the color within the dataset. So, popular colors = black, blue, silver, white
```{r}
cars_data <- cars_data %>%
  mutate(
    has_pop_color = ifelse((color == 'black' | color == 'blue' | color == 'silver'| color == 'white'), 1, 0))
```


- Recode engine type column
```{r}
table(cars_data$engine_fuel)
```

```{r}
table(cars_data$engine_fuel, cars_data$engine_type)
```

Engine fuel and engine type shows the same thing, so engine_type will be used. 

Engine type is recoded as is_diesel, 1 if diesel, 0 if gasoline. 

```{r}
cars_data <- cars_data %>%
  mutate(is_diesel = ifelse(engine_type == 'diesel', 1, 0))
```

Examining body type column
```{r}
boxplot(log(price_usd)~body_type, data = cars_data)
```

```{r}
table(cars_data$body_type)
```

- Recode body_type column as is_big_type (1 if the car body type is big)
Big Cars = Limousine, Minibus, Minivan, Van 

```{r}
cars_data <- cars_data %>% mutate(
  is_big_type = ifelse((body_type=='limousine'|body_type=='minibus'|body_type=='minivan'|body_type=='van'), 1, 0))
```

```{r}
boxplot(log(price_usd)~is_big_type, data = cars_data)
```


manufacturer_name, year_produced, transmission, color, engine_type, and body_type columns are recoded. Therefore, they will be removed. 
engine_fuel, model_name, location_region, and feature 1_to_9 will not be used in the analysis, Therefore, these columns also will be removed.

```{r}
df = cars_data[-c(1, 2, 3, 4, 6, 7, 9, 11, 17, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29)] 
```

```{r}
str(df)
```

# Regression Models

## Linear Regression (LM) 
outcome:price
predictors: engine_capacity, some categorical variables (transmission_is_automatic, has_pop_color, state, drivetrain, is_diesel)

```{r}
lm.1 <- lm(log(price_usd) ~ engine_capacity + transmission_is_automatic + has_pop_color + state +drivetrain + is_diesel , data = df)
summary(lm.1)
```

base level for state= emergency
base level for drivetrain = all 


Since the response variable is log-transformed, we will take exponential of coefficient to interpret the results. 

```{r}
exp(coef(lm.1))
```

### Post-hoc contrast

Testing all pairwise comparisons

```{r message=FALSE}
#install.package("multcomp")
library(multcomp)
ph.test.THSD <- glht(lm.1, linfct = mcp(state = "Tukey", drivetrain = "Tukey"))
summary(ph.test.THSD)
```

All pairwise comparisons are significant ($p < .001$).

### measures of fit
R-Squared

```{r}
summary(lm.1)$adj.r.squared
```
$R^2$ value (0.36) indicates that 36% variability in price of a car can be explained with our model. 


NOTE: Perhaps can be also added an interaction model

## Non-linear models(used GAM)

outcome: price
predictors: all of above + s(age), s(odometer value), s(number of photos)


Graphical examination
```{r fig.width=12, fig.height=8}
library(ggplot2)
library(gridExtra)
gg.engine_capacity <- ggplot(data = df, mapping = aes(y = log(price_usd), x = engine_capacity)) + geom_point() + geom_smooth()
gg.age <- ggplot(data = df, mapping = aes(y = log(price_usd), x = age)) + geom_point() + geom_smooth()
gg.odometer_value <- ggplot(data = df, mapping = aes(y = log(price_usd), x = odometer_value)) + geom_point() + geom_smooth()
gg.number_of_photos <- ggplot(data = df, mapping = aes(y = log(price_usd), x = number_of_photos)) + geom_point() + geom_smooth()
grid.arrange(gg.engine_capacity, gg.age, gg.odometer_value, gg.number_of_photos, ncol = 2)
```


```{r message=FALSE}
library(mgcv)
gam.1 <- gam(log(price_usd) ~ s(engine_capacity) + s(age) + s(odometer_value) + s(number_of_photos) + transmission_is_automatic + has_pop_color + state +drivetrain + is_diesel , data = df)
summary(gam.1)
```

Adjusted $R^2$ value (0.82) indicates that 82% variability in price of a car can be explained with our model.

```{r fig.width=12, fig.size=8}
plot(gam.1, page=1, rug = TRUE, residuals = TRUE, shade = TRUE, shade.col = 'lightblue')
```


#GLM for Binary

1. Recode price variable as dummy variable "is expensive"
If the price of the car is equal or higher than 75% of all car prices: expensive (1). Otherwise, not expensive (0)

```{r}
summary(df$price_usd)
```

```{r}
df$is_expensive = ifelse(df$price_usd >= 8950, 1, 0)
head(df[c("price_usd", "is_expensive")])
```

```{r}
glm.logistic <- glm(is_expensive ~ age + engine_capacity + number_of_photos + odometer_value + transmission_is_automatic + state + drivetrain + is_diesel + has_pop_color , family = "binomial", data = df)
summary(glm.logistic)
```

```{r}
with(summary(glm.logistic), 1 - deviance/null.deviance)
```

#GLMs for Count

Number of photos is a count data, so it will be used as an outcome variable
The possible predictors that may have an effect: is_luxus, has_pop_color, age, price

family: poisson

```{r}
glm.no_of_photos <- glm(number_of_photos ~  price_usd + age + is_luxus + has_pop_color, data = df, family = "poisson")
summary(glm.no_of_photos)
```

Poisson model use natural logarithm by calculating the fit model. Therefore, the exponentials of the coefficients will be interpreted

```{r}
exp(coef(glm.no_of_photos))
```


## SVM

```{r}
str(df)
```

### Prepare the data for training

Recoding state and drivetrain columns as dummy. 

```{r}
df <- df %>% mutate(
  state.emergency = ifelse(state == 'emergency', 1, 0),
  state.new = ifelse(state == 'new', 1, 0),
  state.owned = ifelse(state == 'owned', 1, 0),
  drivetrain.all = ifelse(drivetrain == 'all', 1, 0),
  drivetrain.front = ifelse(drivetrain == 'front', 1, 0),
  drivetrain.rear = ifelse(drivetrain == 'rear', 1, 0)
)
```

Recategorize price into 3 categories

```{r}
df = df %>%
  mutate(price_cat=case_when(
    price_usd <= 2100 ~ "cheap",
    (price_usd > 2100 & price_usd < 8950) ~ "normal",
    price_usd >= 8950 ~ "expensive"
  ))
```


save outcome variables price(continous), is_expensive (binary), price_cat (3 categories)
Save predictors as a separate dataframe. 
```{r}
price <- df$price_usd
price.binary <- df$is_expensive
price.cat <- df$price_cat
x <- df[-c(5, 6, 7, 18, 25)]
```



```{r}
str(x)
```


```{r message=FALSE, warning=FALSE}
# set pseudorandom number generator
set.seed(123)
# Attach Packages
library(tidyverse) # data manipulation and visualization
#library(kernlab) # SVM methodology
library(e1071) # SVM methodology
library(RColorBrewer) # customized coloring of plots
library(caret)
```



```{r}
data.1 <- data.frame(x=as.matrix(x), y=as.factor(price.binary))
```

Split train and test 
```{r}
set.seed(123)
intrain<-createDataPartition(y=data.1$y,p=0.8,list=FALSE)
training<-data.1[intrain,]
testing<-data.1[-intrain,]
```

save y_truth as a separate vector and delete it from testing set
```{r}
y_truth = testing$y
testing <- testing[-c(21)]
```

Try three different model
```{r}
kernfit <- svm(y~.,data=training, kernel = 'radial', cost=0.1, gamma=0.1)
kernfit
```


```{r}
ypred <- predict(kernfit, testing)
confusionMatrix(table(ypred, y_truth))
```


```{r}
kernfit2 <- svm(y~.,data=training, kernel = 'radial', cost=0.1) #use default gamma value (1/nvariables)
kernfit2
```



```{r}
ypred2 <- predict(kernfit2, testing)
confusionMatrix(table(ypred2, y_truth))
```


```{r}
kernfit3 <- svm(y~.,data=training, kernel = 'radial', cost=1)
kernfit3
```


```{r}
ypred3 <- predict(kernfit3, testing)
confusionMatrix(table(ypred3, y_truth))
```

The last one seems to have better accuracy levels (0.93 accuracy, 0.96 sensitivity, and 0.85 specificity) than the other two model. The result is confirmed with tune function below. 


```{r}
# tune model to find optimal cost, gamma values
tune.out <- tune(svm, y~., data = training, kernel = "radial",
                 ranges = list(cost = c(0.01, 0.1, 1)))
# show best model
tune.out$best.model
```

###Predict price category (3 levels)

```{r}
data.2 <- data.frame(x=as.matrix(x), y=as.factor(price.cat))
```

Split train and test 
```{r}
set.seed(123)
intrain2<-createDataPartition(y=data.2$y,p=0.8,list=FALSE)
training2<-data.2[intrain2,]
testing2<-data.2[-intrain2,]
```

save y_truth as a separate vector and delete it from testing set
```{r}
y_truth2 = testing2$y
testing2 <- testing2[-c(21)]
```


Choose the best cost value
```{r}
# tune model to find optimal cost, gamma values
tune.out2 <- tune(svm, y~., data = training2, kernel = "radial",
                  ranges = list(cost = c(0.01, 0.1, 1)))
# show best model
tune.out2$best.model
```


```{r}
kernfit_dat2 <- svm(y~.,data=training2, kernel = 'radial')
kernfit_dat2
```


```{r}
ypred_dat2 <- predict(kernfit_dat2, testing2)
confusionMatrix(table(ypred_dat2,y_truth2))
```


-Cross validation can be added. 

## ANN
```{r message=FALSE}
library(neuralnet)
library(nnet)
library(dplyr)
```

Will be used the same training and testing sets that are created prior to SVM modeling. 

```{r}
set.seed(123)
cars_net <- nnet(y ~ ., training, size=15, maxit=100, range=0.1, decay=5e-4)
```

```{r}
pred <- predict(cars_net, testing, type="class")
confusionMatrix(y_truth, as.factor(pred))
```


```{r}
cars_net2 <- neuralnet(y ~ ., training, hidden = c(1,2))
```



```{r fig.width=12, fig.height=8}
plot(cars_net2)
```


make predictions
```{r}
test_results <- neuralnet::compute(cars_net2, testing)
```



Find class (i.e. output neuron) with the highest probability and convert this back into a factor
```{r}
test_pred <- apply(test_results$net.result, 1, which.max)
test_pred <- factor(levels(y_truth)[test_pred], levels = levels(y_truth))
```

```{r}
confusionMatrix(y_truth, test_pred)
```

#prediction with 3 categories

```{r}
set.seed(123)
cars_net <- nnet(y ~ ., training2, size=15, maxit=100, range=0.1, decay=5e-4)
```

```{r}
pred <- predict(cars_net, testing2, type="class")
confusionMatrix(y_truth2, as.factor(pred))
```

```{r}
cars_net2 <- neuralnet(y ~ ., training2, hidden = c(2,3))
```


```{r fig.width=12, fig.height=8}
plot(cars_net2)
```


make predictions
```{r}
test_results <- neuralnet::compute(cars_net2, testing2)
```



Find class (i.e. output neuron) with the highest probability and convert this back into a factor
```{r}
test_pred <- apply(test_results$net.result, 1, which.max)
test_pred <- factor(levels(y_truth2)[test_pred], levels = levels(y_truth))
```

```{r}
confusionMatrix(y_truth, test_pred)
```
## ABM - Exercise

```{r message = FALSE}
library(NetLogoR)
library(stringr)
library(ggplot2)
library(minpack.lm)
```


```{r}
pubs <- seq(1, 20)
agents <- seq(40, 100, by=1)
simtime<-225# duration time of the simulation
gridSize_x<-15 # number of patches in the grid where moving agents move around
gridSize_y<-15
displacement_normal<-0.1 # speed of moving agents 
displacement_pub<-0.01 # if in the pub, agents move slower and spend more time there
```


```{r message=FALSE}
for (i in agents){
  for (j in pubs){
    
    number_agents<-i
    number_pubs<-j 
    plot_data_out<-numeric() 
    w1 <- createWorld(minPxcor = 0, maxPxcor = gridSize_x-1, minPycor = 0, maxPycor = gridSize_y-1) 
    x_pub<-randomPxcor(w1,number_pubs) 
    y_pub<-randomPycor(w1,number_pubs)
    w1 <- NLset(world = w1, agents = patches(w1), val = 0) 
    w1 <- NLset(world = w1, agents = patch(w1, x_pub, y_pub), val = 1)
    t1 <- createTurtles(n = number_agents, coords = randomXYcor(w1, n = number_agents), breed="S", color="black") 
    t1 <- NLset(turtles = t1, agents = turtle(t1, who = 0), var = "breed", val = "I") 
    t1 <- NLset(turtles = t1, agents = turtle(t1, who = 0), var = "color", val = "red")
    t1 <- turtlesOwn(turtles = t1, tVar = "displacement", tVal = displacement_normal) 
    
    for (time in 1:simtime) { 
      
      t1 <- fd(turtles = t1, dist=t1$displacement, world = w1, torus = TRUE, out = FALSE) 
      t1 <- right(turtles = t1, angle = sample(-20:20, 1, replace = F)) 
      meet<-turtlesOn(world = w1, turtles = t1, agents = t1[of(agents = t1, var = "breed")=="I"]) 
      t1 <- NLset(turtles = t1, agents = meet, var = "breed", val = "I") 
      t1 <- NLset(turtles = t1, agents = meet, var = "color", val = "red") 
      pub <- turtlesOn(world = w1, turtles = t1, agents = patch(w1, x_pub, y_pub)) 
      t1 <- NLset(turtles = t1, agents = turtle(t1, who = pub$who), var = "displacement", val = displacement_pub)
      t1 <- NLset(turtles = t1, agents = turtle(t1, who = t1[-(pub$who+1)]$who), var = "displacement", val = displacement_normal)
      Sys.sleep(0.1) 
      contaminated_counter<-sum(str_count(t1$color, "red"))
      tmp_data<-c(time,contaminated_counter)
      plot_data_out<-rbind(plot_data_out, tmp_data) # store in a matrix
      
    }
    
    df<-as.data.frame(plot_data_out)
    names(df)<-c("time","contaminated_counter")
    x  <- df$time
    y  <- df$contaminated_counter
    model <- nlsLM(y ~ d + (a-d) / (1 + (x/c)^b) ,start = list(a = 3, b = 4, c = 600, d = 1000)) 
    varied_params <- c(number_agents,number_pubs)
    summary_stat <- c( varied_params, as.vector(model$m$getPars()) )
    write.table(as.data.frame(t(summary_stat)), "./data/summary_stat.csv", sep = ",", col.names = FALSE, row.names=FALSE, append = TRUE) 
  }
}
```
```{r}
output <- read.csv("./data/summary_stat.csv", header = FALSE)
```

```{r}
str(output)
```
```{r}
head(output[c(1,2)])
```

```{r}
write.table(as.data.frame(output[c(1,2)]), file = "./data/sim_param.dat", col.names = FALSE, row.names = FALSE, append = FALSE)
write.table(as.data.frame(output[c(3,4,5,6)]), file = "./data/sim_data.dat", col.names = FALSE, row.names = FALSE, append = FALSE)
write.table(as.data.frame(t(c(3, 2, 1143, 1655))), file = "./data/obs_data.dat", col.names = FALSE, row.names = FALSE, append = FALSE)
```

## ABC - Exercises


```{r warning=FALSE, message=FALSE}
#install.packages("abc")
library(abc)
```

Import simulation and observed data
```{r}
sim_param <- read.table(file = "./data/sim_param.dat", header = FALSE)
sim_data <- read.table(file = "./data/sim_data.dat", header = FALSE)
obs_data <- read.table(file = "./data/obs_data.dat", header = FALSE)
```



```{r}
res <- abc(target = obs_data,
           param = sim_param,
           sumstat = sim_data,
           tol = 0.005,
           transf = c("log"),
           method = "neuralnet")
```

```{r}
res$adj.values
```



```{r fig.width=12, fig.height=8}
plot(res, param = sim_param)
```

```{r}
write.table(res$adj.values, "./data/out_abc.tsv", sep = "\t", row.name = FALSE)
```
